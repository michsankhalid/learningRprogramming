---
title: "Statistics Fundamental"
output: html_notebook
---

# Load Library
```{r message=FALSE,warning=FALSE}
library(tidyverse)
library(skimr)
library(janitor)
library(ggthemr)
library(tidytext)
library(esquisse)
library(plotly)
library(DataExplorer)
library(ff)
library(kableExtra)
library(lubridate)
library(tidyquant)
```

# Introduction to Statistics

## Load Data
```{r message=FALSE,warning=FALSE}
food_consumption <- read_rds("food_consumption.rds")
amir_deals <- read_rds("seller_1.rds")
```

Cek struktur data
```{r message=FALSE,warning=FALSE}
glimpse(food_consumption)
glimpse(amir_sales)
```

## Measures of Center

- If data not skew, use mean
- If data skew (right or left), use median

## Measures of Spread

- Variance : Average jarak data ke rata-rata. Makin besa nilainya, makin spread datanya
- Standar Deviasi : akar dari variance
- Quartiles
- Quantiles : Versi fleksibelnya quartiles
- Quartiles bisa menghasilkan IQR (Q3 - Q1). IQR bisa mendeteksi outliers.
- Rules of outliers : 
  - Lower treshold : Q1 - IQR*1.5
  - Upper treshold : Q3 + IQR*1.5
- Quantiles are a great way of summarizing numerical data since they can be used to measure center and spread, as well as to get a sense of where a data point stands in relation to the rest of the data set. For example, you might want to give a discount to the 10% most active users on a website.
```{r}
# Calculate the quartiles of co2_emission
quantile(food_consumption$co2_emission)

# Calculate the quintiles of co2_emission
quantile(food_consumption$co2_emission,probs = seq(0,1,0.2))

# Calculate the deciles of co2_emission
quantile(food_consumption$co2_emission, probs = seq(0,1,0.1))
```
- Variance and standard deviation are two of the most common ways to measure the spread of a variable, and you'll practice calculating these in this exercise. Spread is important since it can help inform expectations. For example, if a salesperson sells a mean of 20 products a day, but has a standard deviation of 10 products, there will probably be days where they sell 40 products, but also days where they only sell one or two.
```{r}
# Calculate variance and sd of co2_emission for each food_category
food_consumption %>% 
  group_by(food_category) %>% 
  summarise(var_co2 = var(co2_emission),
     sd_co2 = sd(co2_emission))

# Plot food_consumption with co2_emission on x-axis
ggplot(food_consumption, aes(x = co2_emission)) +
  # Create a histogram
  geom_histogram() +
  # Create a separate sub-graph for each food_category
  facet_wrap(~ food_category) + theme_tq()
```
- Outliers can have big effects on statistics like mean, as well as statistics that rely on the mean, such as variance and standard deviation. Interquartile range, or IQR, is another way of measuring spread that's less influenced by outliers. IQR is also often used to find outliers. If a value is less than **Q1 - IQR*1.5** or greater than **Q3 + IQR*1.5**, it's considered an outlier.
```{r}
# Calculate total co2_emission per country: emissions_by_country
emissions_by_country <- food_consumption %>%
  group_by(country) %>%
  summarize(total_emission = sum(co2_emission))

# Compute the first and third quantiles and IQR of total_emission
q1 <- quantile(emissions_by_country$total_emission, 0.25)
q3 <- quantile(emissions_by_country$total_emission, 0.75)
iqr <- q3 - q1

# Calculate the lower and upper cutoffs for outliers
lower <- q1 - 1.5 * iqr
upper <- q3 + 1.5 * iqr

# Filter emissions_by_country to find outliers
emissions_by_country %>%
  filter(total_emission < lower | total_emission > upper)
```
## Chances
### Calculating Probabilities
- Independent events : Two events are independent if the probability of the second event isn't afected by the outcome of the first event
- Dependent events : Two events are dependent if the probability of the second event is afected by the outcome of the first event.

```{r}
# Calculate probability of picking a deal with each product
amir_deals %>%
  count(product) %>%
  mutate(prob = n/sum(n))
```
### Sampling Deals
Additionally, you want to make sure this is done randomly and that it can be reproduced in case you get asked how you chose the deals, so you'll need to set the random seed before sampling from the deals.
```{r}
# Set random seed to 31
set.seed(31)

# Sample 5 deals without replacement
amir_deals %>%
  sample_n(5)

# Sample 5 deals with replacement
amir_deals %>%
  sample_n(5, replace = TRUE)
```
## Discrete Distribution
### Creating a probability distribution
- Expected value : mean of a probability distribution
```{r}
restaurant_groups <- structure(list(group_id = structure(1:10, .Label = c("A", "B", 
"C", "D", "E", "F", "G", "H", "I", "J"), class = "factor"), group_size = c(2, 
4, 6, 2, 2, 2, 3, 2, 4, 2)), .Names = c("group_id", "group_size"
), row.names = c(NA, -10L), class = "data.frame")

## Create a histogram of group_size
ggplot(restaurant_groups, aes(x = group_size)) +
  geom_histogram(bins = 5)

## Create probability distribution
size_distribution <- restaurant_groups %>%
  # Count number of each group size
  count(group_size) %>%
  # Calculate probability
  mutate(probability = n / sum(n))

size_distribution

## Create probability distribution
size_distribution <- restaurant_groups %>%
  count(group_size) %>%
  mutate(probability = n / sum(n))

# Calculate expected group size
expected_val <- sum(size_distribution$group_size *
                    size_distribution$probability)
expected_val

# Calculate probability of picking group of 4 or more
size_distribution %>%
  # Filter for groups of 4 or larger
  filter(group_size >= 4) %>%
  # Calculate prob_4_or_more by taking sum of probabilities
  summarize(prob_4_or_more = sum(probability))


```
## Continuous Distribution
### Data back up
The sales software used at your company is set to automatically back itself up, but no one knows exactly what time the back-ups happen. It is known, however, that back-ups happen exactly every 30 minutes. Amir comes back from sales meetings at random times to update the data on the client he just met with. He wants to know how long he'll have to wait for his newly-entered data to get backed up. Use your new knowledge of continuous uniform distributions to model this situation and answer Amir's questions.
```{r}
# Min and max wait times for back-up that happens every 30 min
min <- 0
max <- 30

# Calculate probability of waiting less than 5 mins
prob_less_than_5 <- punif(5,0,30)
prob_less_than_5

# Calculate probability of waiting more than 5 mins
prob_greater_than_5 <- punif(5,0,30,lower.tail = FALSE)
prob_greater_than_5

# Calculate probability of waiting 10-20 mins
prob_between_10_and_20 <- punif(20,0,30) - punif(10,0,30)
prob_between_10_and_20
```
### Simulating wait times
To give Amir a better idea of how long he'll have to wait, you'll simulate Amir waiting 1000 times and create a histogram to show him what he should expect. Recall from the last exercise that his minimum wait time is 0 minutes and his maximum wait time is 30 minutes.
```{r}
wait_times <- structure(list(simulation_nb = 1:1000), .Names = "simulation_nb", row.names = c(NA, 
-1000L), class = c("tbl_df", "tbl", "data.frame"))

# Set random seed to 334
set.seed(334)

# Generate 1000 wait times between 0 and 30 mins, save in time column
wait_times %>%
  mutate(time = runif(1000, min = 0, max = 30)) %>%
  # Create a histogram of simulated times
  ggplot(aes(x = time)) +
  geom_histogram()
```
## Binomial Distribution
### Simulating sales deals
Assume that Amir usually works on 3 deals per week, and overall, he wins 30% of deals he works on. Each deal has a binary outcome: it's either lost, or won, so you can model his sales deals with a binomial distribution. In this exercise, you'll help Amir simulate a year's worth of his deals so he can better understand his performance.
```{r}
# Set random seed to 10
set.seed(10)

# Simulate a single deal
rbinom(1, 1, 0.3)

# Simulate 1 week of 3 deals
rbinom(1,3,0.3)

# Simulate 52 weeks of 3 deals
deals <- rbinom(52,3,0.3)

# Calculate mean deals won per week
mean(deals)
```
### Calculating binomial probabilities
Just as in the last exercise, assume that Amir wins 30% of deals. He wants to get an idea of how likely he is to close a certain number of deals each week. In this exercise, you'll calculate what the chances are of him closing different numbers of deals using the binomial distribution.
```{r}
# Probability of closing 3 out of 3 deals
dbinom(3,3,0.3)

# Probability of closing <= 1 deal out of 3 deals
pbinom(1,3,0.3)

# Probability of closing > 1 deal out of 3 deals
pbinom(1,3,0.3,lower.tail = FALSE)
```
## Normal Distribution
### Probabilities from the normal distribution
Since each deal Amir worked on (both won and lost) was different, each was worth a different amount of money. These values are stored in the `amount` column of `amir_deals` and follow a normal distribution with a mean of 5000 dollars and a standard deviation of 2000 dollars. As part of his performance metrics, you want to calculate the probability of Amir closing a deal worth various amounts.
```{r}
# Probability of deal < 7500
pnorm(7500,mean = 5000,sd = 2000)

# Probability of deal > 1000
pnorm(1000,5000,2000,lower.tail = FALSE)

# Probability of deal between 3000 and 7000
pnorm(7000,5000,2000) - pnorm(3000,5000,2000)

# Calculate amount that 75% of deals will be more than
qnorm(0.75,mean = 5000,sd = 2000,lower.tail = FALSE)
```
### Simulating sales under new market conditions
The company's financial analyst is predicting that next quarter, the worth of each sale will increase by 20% and the volatility, or standard deviation, of each sale's worth will increase by 30%. To see what Amir's sales might look like next quarter under these new market conditions, you'll simulate new sales amounts using the normal distribution and store these in the `new_sales` data frame
```{r}
new_sales <- structure(list(sale_num = 1:36), .Names = "sale_num", row.names = c(NA, 
-36L), class = "data.frame")

# Calculate new average amount
new_mean <- 5000 * 1.2

# Calculate new standard deviation
new_sd <- 2000 *1.3

# Simulate 36 sales
new_sales <- new_sales %>% 
  mutate(amount = rnorm(36,new_mean,new_sd))

# Create histogram with 10 bins
ggplot(new_sales, aes(x = amount)) + geom_histogram(bins = 10)
```
### Which market is better?
The key metric that the company uses to evaluate salespeople is the percent of sales they make over $1000 since the time put into each sale is usually worth a bit more than that, so the higher this metric, the better the salesperson is performing.

Recall that Amir's current sales amounts have a mean of $5000 and a standard deviation of $2000, and Amir's predicted amounts in next quarter's market have a mean of $6000 and a standard deviation of $2600.

Based only on the metric of percent of sales over $1000, does Amir perform better in the current market or the predicted market?
```{r}
pnorm(1000, mean = 5000, sd = 2000, lower.tail = FALSE)
pnorm(1000, mean = 6000, sd = 2500, lower.tail = FALSE)
```
Both of them have good predicted market

## Central Limit Theorem (CLT)
which states that a sampling distribution will approach a normal distribution as the number of trials increases. In our example, the sampling distribution became closer to the normal distribution as we took more and more sample means. It's important to note that the central limit theorem only applies when samples are taken randomly and are independent, for example, randomly picking sales deals with replacement.
- Sampling Distribution : This distribution, specifically, is a sampling distribution of the sample mean.
### The CLT in action
The central limit theorem states that a sampling distribution of a sample statistic approaches the normal distribution as you take more samples, no matter the original distribution being sampled from.
```{r}
# Create a histogram of num_users
ggplot(amir_deals,aes(x = num_users)) + geom_histogram(bins = 10)

# Set seed to 104
set.seed(104)

# Sample 20 num_users from amir_deals and take mean
sample(amir_deals$num_users, size = 20, replace = TRUE) %>%
  mean()

# Repeat the above 100 times
sample_means <- replicate(100, sample(amir_deals$num_users, size = 20, replace = TRUE) %>% mean())

# Create data frame for plotting
samples <- data.frame(mean = sample_means)

# Histogram of sample means
ggplot(samples,aes(x = mean)) +
  geom_histogram(bins = 10)
```
### The mean of means
You want to know what the average number of users (num_users) is per deal, but you want to know this number for the entire company so that you can see if Amir's deals have more or fewer users than the company's average deal. The problem is that over the past year, the company has worked on more than ten thousand deals, so it's not realistic to compile all the data. Instead, you'll estimate the mean by taking several random samples of deals, since this is much easier than collecting data from everyone in the company.
```{r}
all_deals <- read_csv("all_deals.csv")

# Set seed to 321
set.seed(321)

# Take 30 samples of 20 values of num_users, take mean of each sample
sample_means <- replicate(30, sample(all_deals$num_users, 20) %>% mean())

# Calculate mean of sample_means
mean(sample_means)

# Calculate mean of num_users in amir_deals
mean(amir_deals$num_users)
```
## Poisson Distribution
- Poisson processess : a process where events appear to happen at a certain rate, but completely at random.
- The Poisson distribution : the probability of some number of events happening over a fixed period of time.
- The Poisson distribution is described by a value called lambda, which represents the average number of events per time period.
### Tracking lead responses
Your company uses sales software to keep track of new sales leads. It organizes them into a queue so that anyone can follow up on one when they have a bit of free time. Since the number of lead responses is a countable outcome over a period of time, this scenario corresponds to a Poisson distribution. On average, Amir responds to 4 leads each day. In this exercise, you'll calculate probabilities of Amir responding to different numbers of leads.
```{r}
 # Probability of 5 responses
dpois(5,4)

# Probability of 5 responses from coworker
dpois(5,5.5)

# Probability of 2 or fewer responses
ppois(2,4)

# Probability of > 10 responses
ppois(10,4,lower.tail = FALSE)
```
## Modeling time between leads
To further evaluate Amir's performance, you want to know how much time it takes him to respond to a lead after he opens it. On average, it takes 2.5 hours for him to respond. In this exercise, you'll calculate probabilities of different amounts of time passing between Amir receiving a lead and sending a response.
```{r}
# Probability response takes < 1 hour
pexp(1,rate = 1/2.5)
# Probability response takes > 4 hours
pexp(4, 1/2.5,lower.tail = FALSE)
# Probability response takes 3-4 hours
pexp(4,1/2.5) - pexp(3,1/2.5)
```
## Correlations
Relationship between two variable, with :
- x = independent variable
- y = dependent variable

Correlation Coefficient
- Quentifies the linear relationship between two variable
- Between -1 to 1
- Magnitude corresponding to strength relationship
- (+ & -) sign of direction

### Relationships between variables
```{r message=FALSE,warning=FALSE}
world_happiness <- read_rds("world_happiness_sugar.rds")
```
```{r}
# Add a linear trendline to scatterplot
ggplot(world_happiness, aes(life_exp, happiness_score)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)

# Correlation between life_exp and happiness_score
cor(world_happiness$life_exp,world_happiness$happiness_score)
```
## Correlation Coveats
Digunakan ketika tidak berkorelasi secara linear antara dua variabel.
Ketika distribusi data skew, bisa menggunakan beberapa transformasi, diantaranya 
- log transformation (log(data))
- square root transformation (sqrt(data))
- reciprocal transformation (1/data)
Correlation hanya menghitung hubungan secara linear, maka transformasi diperlukan

### What can't correlation measure?
```{r}
# Scatterplot of gdp_per_cap and life_exp
ggplot(world_happiness, aes(gdp_per_cap, life_exp)) +
  geom_point()

# Correlation between gdp_per_cap and life_exp
cor(world_happiness$gdp_per_cap,world_happiness$life_exp)
```
Meski punya coefficient correlation yang tinggi, namun tidak bisa digunakan. Karena tidak berkolerasi linear

### Transforming variables
When variables have skewed distributions, they often require a transformation in order to form a linear relationship with another variable so that correlation can be computed. 
```{r}
# Create log_gdp_per_cap column
world_happiness <- world_happiness %>%
  mutate(log_gdp_per_cap = log(gdp_per_cap))

# Scatterplot of log_gdp_per_cap vs. happiness_score
ggplot(world_happiness, aes(log_gdp_per_cap, happiness_score)) +
  geom_point() + theme_tq()

# Calculate correlation
cor(world_happiness$log_gdp_per_cap, world_happiness$happiness_score)
```
### Does sugar improve happiness?
```{r}
# Scatterplot of grams_sugar_per_day and happiness_score
ggplot(world_happiness,aes(grams_sugar_per_day,happiness_score)) + geom_point()

# Correlation between grams_sugar_per_day and happiness_score
cor(world_happiness$grams_sugar_per_day,world_happiness$happiness_score)
```
Increased sugar consumption is associated with a higher happiness score.
## Design of experiments
**Vocabulary**
Goals : to answer efek dari treatment
- Treatment : independent variable
- Response : dependent variable

Contoh : apa efek dari iklan terhadap jumlah barang yang terbeli?
- Treatment : iklan
- Response : barang yang terbeli
**Controlled Experiment**
- Partisipan terbagi kedalam controll dan treatment group
  - Treatment group : liat iklan
  - Controll group : ga liat iklan
- Group harus comparable, jika tidak akan ada bias

# Introduction to Regression in R
## Two Variable
**What is regression?**
- Statistical models to explore there relationship a response variable and some explanatory variables.
- Given values of explanatory variables,you can predict the values of the response variable.



